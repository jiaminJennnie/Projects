pred=predict(mod,newdata = data.frame(test.X),type = "response")
glm.pred=rep(0,1000)
glm.pred[pred>0.25]=1
table(test.Y)
table(glm.pred,test.Y)
id=which(test.Y==1)
library(pROC)
#27 6 59 0.497
library(glmnet)
mod4=glmnet(train.X,train.Y,family="binomial",alpha=1, lambda = 0.004)
phat3=predict(mod4, newx = test.X,type = "response")
length(coef(mod4)!=NA)
glm.pred4=rep(0,1000)
glm.pred4[phat3>0.25]=1
table(test.Y)
table(glm.pred4,test.Y)
r3=roc(response=test.Y,predictor=glm.pred4)
auc(r3)
mod5=glmnet(train.X,train.Y,alpha=1)
mycoef = predict(mod5, x= train.X, y= train.Y,
s=0.004, type="coefficients", exact=TRUE,
thresh = 1e-08)
phat4=predict(mycoef, newx = test.X,type = "response")
performance(phat3,"auc")
pred <- prediction(phat3, test.Y)
library(ROCR)
pred <- prediction(phat3, test.Y)
perf=performance(pred,"auc")
perf
cvfit=cv.glmnet(train.X,train.Y,family = "binomial", type.measure = "auc")
mycoef = predict(mod5, x= train.X, y= train.Y,
s=0.004, type="coefficients", exact=TRUE,
thresh = 1e-08)
phat4=predict(mycoef, newx = test.X,type = "response")
auc(r3)
r3=roc(response=test.Y,predictor=glm.pred4)
r3
r2
mod4
table(test.Y)
length(coef(mod4)!=NA)
length(coef(mod4)!=0)
mod4=glmnet(train.X,train.Y,family="binomial",alpha=1, lambda = 0.004)
length(coef(mod4)!=0)
length(coef(mod4)==0)
mod4$beta
length(mod4$beta!=0)
length(mod4$beta!=NA)
length(mod4$beta[,2]  ! =NA)
length(mod4$beta[2,]  ! =NA)
length(mod4$beta[,2]  !=NA)
mod4$a0
r=roc(response=test$Purchase,predictor=glm.pred)
data("Caravan")
test<-Caravan[1:1000,]
train<-Caravan[-(1:1000),]
logmodel<-glm(Purchase~.,data=train, family = binomial(link='logit'))
pred=predict(logmodel,newdata = test,type = "response")
glm.pred=rep(0,1000)
glm.pred[pred>0.25]=1
table(test$Purchase)
table(glm.pred,test$Purchase)
fit2 <- glm(Purchase~1,data=train ,family=binomial)
stepMod<-step(fit2,direction='forward',scope=list(upper=logmodel,lower=fit2))
r=roc(response=test$Purchase,predictor=glm.pred)
auc(r)
r
r=roc(response=test$Purchase,predictor=glm.pred)
r
pred=predict(StepMod,newdata = test,type = "response")
stepMod<-step(fit2,direction='forward',scope=list(upper=logmodel,lower=fit2))
r=roc(response=test.Y,predictor=glm.pred)
pred=predict(StepMod,newdata = test,type = "response")
pred=predict(stepMod,newdata = test,type = "response")
pred1=predict(stepMod,newdata = test,type = "response")
r1=roc(response=test$Purchase,predictor=pred1)
r1
pred1
#quiz8
install.packages("ISLR")
install.packages("ISLR")
library(ISLR)
data(Caravan)
str(Caravan)
table(Caravan$Purchase)
test=Caravan[1:1000,]
train=Caravan[-(1:1000),]
table(test$Purchase)
Caravan$Purchase=as.numeric(Caravan$Purchase)
Caravan$Purchase=Caravan$Purchase-1
str(Caravan$Purchase)()
standard.x=scale(Caravan[,-86])
test.X=standard.x[1:1000,]
train.X=standard.x[-(1:1000),]
test.Y=Caravan$Purchase[1:1000]
train.Y=Caravan$Purchase[-(1:1000)]
train=data.frame(train.X,train.Y)
test=data.frame(test.X,test.Y)
mod=glm(train.Y~.,binomial(link='logit'),data=train)
pred=predict(mod,newdata = data.frame(test.X),type = "response")
glm.pred=rep(0,1000)
glm.pred[pred>=0.25]=1
table(test.Y)
table(glm.pred,test.Y)
id=which(test.Y==1)
table(glm.pred[id])
install.packages("pROC")
install.packages("pROC")
library(pROC)
r=roc(response=test.Y,predictor=glm.pred)
auc(r)
r
train$Purchase
test<-Caravan[1:1000,]
train<-Caravan[-(1:1000),]
train$Purchase
logmodel<-glm(Purchase~.,data=train, family = binomial)
pred=predict(logmodel,newdata = test)
head(pred)
glm.pred=rep(0,1000)
glm.pred[pred>0.25]=1
table(test$Purchase)
table(glm.pred,test$Purchase)
pred=predict(logmodel,newdata = test,type='response')
glm.pred=rep(0,1000)
glm.pred[pred>0.25]=1
table(test$Purchase)
table(glm.pred,test$Purchase)
mod4=glmnet(Purchase~.,data=train,family="binomial",alpha=1, lambda = 0.004)
train[,-Purchase]
train$Purchase
colnames(train[,86])
rownames(train[,86])
View(test)
mod4=glmnet(Y=train$Purchase,X=train[,-86],family="binomial",alpha=1, lambda = 0.004)
mod4=glmnet(y=train$Purchase,x=train[,-86],family="binomial",alpha=1, lambda = 0.004)
X=data.matrix(train[,-86])
mod4=glmnet(X,y=train$Purchase,family="binomial",alpha=1, lambda = 0.004)
phat3=predict(mod4, newx = test,type = "response")
Test<-data.matrix(test[,-86])
phat3=predict(mod4, newx = Test,type = "response")
length(mod4$beta[,2]  !=NA)
length(mod4$beta!=NA)
mod4$beta
glm.pred4=rep(0,1000)
glm.pred4[phat3>0.25]=1
table(test.Y)
table(glm.pred4,test.Y)
r3=roc(response=test.Y,predictor=glm.pred4)
r3
table(glm.pred4,test$Purchase)
#Data Prepocessing--------------------------------------------------------------
setwd("~/Desktop/Study/542")
data <- read.csv('Ames_data.csv', head = T)
load("/Users/jiamin/Downloads/project1_testIDs.RData")
#Missing Data------------------------------------------------
missing.n = sapply(names(data), function(x) length(which(is.na(data[, x]))))
which(missing.n > 0)  # 60th col: Garage_Yr_Blt
id = which(is.na(data$Garage_Yr_Blt))
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
all = read.table("Project2_data.tsv",stringsAsFactors = F,header = T)
all$review = gsub('<.*?>', ' ', all$review)
splits = read.table("Project2_splits.csv", header = T)
s = 1 # Here we use the 1st training/test split.
train = all[-which(all$new_id%in%splits[,s]),]
test = all[which(all$new_id%in%splits[,s]),]
#------------------------------------------Creating bags of words
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$review,
preprocessor = prep_fun,
tokenizer = tok_fun)
it_test = itoken(test$review,
preprocessor = prep_fun,
tokenizer = tok_fun)
stop_words = c("i", "me", "my", "myself",
"we", "our", "ours", "ourselves",
"you", "your", "yours",
"their", "they", "his", "her",
"she", "he", "a", "an", "and",
"is", "was", "are", "were",
"him", "himself", "has", "have",
"it", "its", "of", "one", "for",
"the", "us", "this")
stop_words = c('a','about',
'above','after','again','against','ain','all','am','an','and','any',
'are','aren',"aren't",'as','at','be','because','been','before',
'being','below','between','both','but','by','can','couldn',"couldn't",
'd','did','didn',"didn't",'do','does','doesn',"doesn't",
'doing','don',"don't",'down','during','each','few','for','from',
'further','had','hadn',"hadn't",'has','hasn',"hasn't",
'have','haven',"haven't",'having','he','her','here','hers',
'herself','him','himself','his','how','i','if','in','into','is',
'isn',"isn't",'it',"it's",'its','itself','just','ll','m','ma',
'me','mightn',"mightn't",'more','most','mustn',"mustn't",
'my','myself','needn',"needn't",'no','nor','not','now',
'o','of','off','on','once','only','or','other','our',
'ours','ourselves','out','over','own','re','s',
'same','shan',"shan't",'she',"she's",
'should',"should've",'shouldn',"shouldn't",'so','some',
'such','t','than','that',"that'll",'the','their',
'theirs','them','themselves','then','there','these',
'they','this','those','through','to','too','under',
'until','up','ve','very','was','wasn',"wasn't",'we','were',
'weren',"weren't",'what','when','where','which',
'while','who','whom','why','will','with','won',
"won't",'wouldn',"wouldn't",'y','you',"you'd","you'll",
"you're","you've",'your','yours','yourself','yourselves'
)
vocab = create_vocabulary(it_train,ngram = c(1L,4L), stopwords = stop_words)
s = 1 # Here we use the 1st training/test split.
train = all[-which(all$new_id%in%splits[,s]),]
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
all = read.table("Project2_data.tsv",stringsAsFactors = F,header = T)
#------------------------------------------Creating triaining and test datasets
setwd("~/Desktop/movie review")
all = read.table("Project2_data.tsv",stringsAsFactors = F,header = T)
all$review = gsub('<.*?>', ' ', all$review)
splits = read.table("Project2_splits.csv", header = T)
s = 1 # Here we use the 1st training/test split.
train = all[-which(all$new_id%in%splits[,s]),]
test = all[which(all$new_id%in%splits[,s]),]
#------------------------------------------Creating bags of words
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$review,
preprocessor = prep_fun,
tokenizer = tok_fun)
it_test = itoken(test$review,
preprocessor = prep_fun,
tokenizer = tok_fun)
stop_words = c('a','about',
'above','after','again','against','ain','all','am','an','and','any',
'are','aren',"aren't",'as','at','be','because','been','before',
'being','below','between','both','but','by','can','couldn',"couldn't",
'd','did','didn',"didn't",'do','does','doesn',"doesn't",
'doing','don',"don't",'down','during','each','few','for','from',
'further','had','hadn',"hadn't",'has','hasn',"hasn't",
'have','haven',"haven't",'having','he','her','here','hers',
'herself','him','himself','his','how','i','if','in','into','is',
'isn',"isn't",'it',"it's",'its','itself','just','ll','m','ma',
'me','mightn',"mightn't",'more','most','mustn',"mustn't",
'my','myself','needn',"needn't",'no','nor','not','now',
'o','of','off','on','once','only','or','other','our',
'ours','ourselves','out','over','own','re','s',
'same','shan',"shan't",'she',"she's",
'should',"should've",'shouldn',"shouldn't",'so','some',
'such','t','than','that',"that'll",'the','their',
'theirs','them','themselves','then','there','these',
'they','this','those','through','to','too','under',
'until','up','ve','very','was','wasn',"wasn't",'we','were',
'weren',"weren't",'what','when','where','which',
'while','who','whom','why','will','with','won',
"won't",'wouldn',"wouldn't",'y','you',"you'd","you'll",
"you're","you've",'your','yours','yourself','yourselves'
)
vocab = create_vocabulary(it_train,ngram = c(1L,4L), stopwords = stop_words)
pruned_vocab = prune_vocabulary(vocab,
term_count_min = 5,
doc_proportion_max = 0.5,
doc_proportion_min = 0.01)
bigram_vectorizer = vocab_vectorizer(pruned_vocab)
dtm_train = create_dtm(it_train, bigram_vectorizer)
dtm_test = create_dtm(it_test, bigram_vectorizer)
v.size = dim(dtm_train)[2]
ytrain = train$sentiment
summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)
summ[,2] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), var)
summ[,3] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==0, ]), mean)
summ[,4] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==0, ]), var)
n1=sum(ytrain);
n=length(ytrain)
n0= n - n1
myp = (summ[,1] - summ[,3])/
sqrt(summ[,2]/n1 + summ[,4]/n0)
words = colnames(dtm_train)
id = order(abs(myp), decreasing=TRUE)[1:3500]
write(words[id], file="myvocab.txt")
#-------------
myvocab = scan(file = "myvocab.txt", what = character())
all = read.table("Project2_data.tsv",stringsAsFactors = F,header = T)
all$review = gsub('<.*?>', ' ', all$review)
splits = read.table("Project2_splits.csv", header = T)
s = 1
train = all[-which(all$new_id%in%splits[,s]),]
test = all[which(all$new_id%in%splits[,s]),]
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
vocab = create_vocabulary(it_train, ngram = c(1L,4L))
vocab = vocab[vocab$term %in% myvocab, ]
bigram_vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, bigram_vectorizer)
dtm_test = create_dtm(it_test, bigram_vectorizer)
#-----------------------------------------------------Logistic
t1 = Sys.time()
lasso = cv.glmnet(x = dtm_train, y = train$sentiment,
family = 'binomial',
alpha = 1,
type.measure = "auc")
tmp <-predict(lasso, s = lasso$lambda.min, newx = dtm_test,type = "response")
auc(test$sentiment, tmp)
#------------------------------------------Creating triaining and test datasets
setwd("~/Desktop/movie review")
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
library(MASS)
all = read.table("Project2_data.tsv",stringsAsFactors = F,header = T)
all$review = gsub('<.*?>', ' ', all$review)
splits = read.table("Project2_splits.csv", header = T)
s = 1 # Here we use the 1st training/test split.
train = all[-which(all$new_id%in%splits[,s]),]
test = all[which(all$new_id%in%splits[,s]),]
#------------------------------------------Creating bags of words
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$review,
preprocessor = prep_fun,
tokenizer = tok_fun)
it_test = itoken(test$review,
preprocessor = prep_fun,
tokenizer = tok_fun)
vocab = create_vocabulary(it_train,ngram = c(1L,4L))
pruned_vocab = prune_vocabulary(vocab,
term_count_min = 5,
doc_proportion_max = 0.5,
doc_proportion_min = 0.01)
bigram_vectorizer = vocab_vectorizer(pruned_vocab)
dtm_train = create_dtm(it_train, bigram_vectorizer)
dtm_test = create_dtm(it_test, bigram_vectorizer)
v.size = dim(dtm_train)[2]
ytrain = train$sentiment
summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)
summ[,2] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==1, ]), var)
summ[,3] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==0, ]), mean)
summ[,4] = colapply_simple_triplet_matrix(
as.simple_triplet_matrix(dtm_train[ytrain==0, ]), var)
n1=sum(ytrain);
n=length(ytrain)
n0= n - n1
myp = (summ[,1] - summ[,3])/
sqrt(summ[,2]/n1 + summ[,4]/n0)
words = colnames(dtm_train)
id = order(abs(myp), decreasing=TRUE)[1:3000]
write(words[id], file="myvocab.txt")
#-------------
myvocab = scan(file = "myvocab.txt", what = character())
all = read.table("Project2_data.tsv",stringsAsFactors = F,header = T)
all$review = gsub('<.*?>', ' ', all$review)
splits = read.table("Project2_splits.csv", header = T)
s = 1
train = all[-which(all$new_id%in%splits[,s]),]
test = all[which(all$new_id%in%splits[,s]),]
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
vocab = create_vocabulary(it_train, ngram = c(1L,4L), stopwords = stop_words)
stop_words = c("i", "me", "my", "myself",
"we", "our", "ours", "ourselves",
"you", "your", "yours",
"their", "they", "his", "her",
"she", "he", "a", "an", "and",
"is", "was", "are", "were",
"him", "himself", "has", "have",
"it", "its", "of", "one", "for",
"the", "us", "this")
vocab = create_vocabulary(it_train, ngram = c(1L,4L), stopwords = stop_words)
vocab = vocab[vocab$term %in% myvocab, ]
bigram_vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, bigram_vectorizer)
dtm_test = create_dtm(it_test, bigram_vectorizer)
lasso = cv.glmnet(x = dtm_train, y = train$sentiment,
family = 'binomial',
alpha = 1,
type.measure = "auc")
tmp <-predict(lasso, s = lasso$lambda.min, newx = dtm_test,type = "response")
auc(test$sentiment, tmp)
#-------------
myvocab = scan(file = "myvocab.txt", what = character())
all = read.table("Project2_data.tsv",stringsAsFactors = F,header = T)
all$review = gsub('<.*?>', ' ', all$review)
splits = read.table("Project2_splits.csv", header = T)
s = 1
train = all[-which(all$new_id%in%splits[,s]),]
test = all[which(all$new_id%in%splits[,s]),]
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
stop_words = c("i", "me", "my", "myself",
"we", "our", "ours", "ourselves",
"you", "your", "yours",
"their", "they", "his", "her",
"she", "he", "a", "an", "and",
"is", "was", "are", "were",
"him", "himself", "has", "have",
"it", "its", "of", "one", "for",
"the", "us", "this")
vocab = create_vocabulary(it_train, ngram = c(1L,4L), stopwords = stop_words)
vocab = vocab[vocab$term %in% myvocab, ]
bigram_vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, bigram_vectorizer)
dtm_test = create_dtm(it_test, bigram_vectorizer)
vocab = vocab[vocab$term %in% myvocab, ]
bigram_vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, bigram_vectorizer)
dtm_test = create_dtm(it_test, bigram_vectorizer)
lasso = cv.glmnet(x = dtm_train, y = train$sentiment,
family = 'binomial',
alpha = 1,
type.measure = "auc")
tmp <-predict(lasso, s = lasso$lambda.min, newx = dtm_test,type = "response")
auc(test$sentiment, tmp)
tmp <-predict(lasso, s = lasso$lambda.1se, newx = dtm_test,type = "response")
auc(test$sentiment, tmp)
table(test$sentiment,tmp)
tmp1<-rep(0,25000)
tmp1[tmp>0.5]=1
auc(test$sentiment, tmp1)
table(tmp1,test$sentiment)
id1<-which(tmp1!=test$sentiment)
write.table('error.txt',all[id1])
all = read.table("Project2_data.tsv",stringsAsFactors = F,header = T)
train = all[-which(all$new_id%in%splits[,s]),]
test = all[which(all$new_id%in%splits[,s]),]
wirte.table('error.txt',test[id1])
write.table('error.txt',test[id1])
test$sentiment[id1]
write.table('error.txt',test[id1,])
write.table(test[id1,],'error.txt')
write.table(dtm_test[id1,],'compare.txt')
dtm_test[id1]
#------------------------------------------Creating triaining and test datasets
setwd("~/Desktop/movie review")
library(text2vec)
library(slam)
library(glmnet)
library(pROC)
library(MASS)
write(words[id], file="myvocab.txt")
myvocab1 = scan(file = "myvocab1.txt", what = character())
#-------------
myvocab = scan(file = "myvocab.txt", what = character())
myvocab_final = myvocab%in%myvocab1
write(myvocab_final, file="myvocab.txt")
#-------------
myvocab = scan(file = "myvocab.txt", what = character())
myvocab2 = scan(file = "myvocab2.txt", what = character())
myvocab_final = myvocab2[which(myvocab2%in%myvocab1)]
write(myvocab_final, file="myvocab.txt")
myvocab3=myvocab1[which(myvocab1!%in%myvocab_final)]
myvocab3=myvocab1[-which(myvocab1%in%myvocab_final)]
write(myvocab_final, file="myvocab4.txt")
#-------------
myvocab = scan(file = "myvocab.txt", what = character())
all = read.table("Project2_data.tsv",stringsAsFactors = F,header = T)
all$review = gsub('<.*?>', ' ', all$review)
splits = read.table("Project2_splits.csv", header = T)
s = 1
train = all[-which(all$new_id%in%splits[,s]),]
test = all[which(all$new_id%in%splits[,s]),]
it_train = itoken(train$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
it_test = itoken(test$review,
preprocessor = tolower,
tokenizer = word_tokenizer)
stop_words = c("i", "me", "my", "myself",
"we", "our", "ours", "ourselves",
"you", "your", "yours",
"their", "they", "his", "her",
"she", "he", "a", "an", "and",
"is", "was", "are", "were",
"him", "himself", "has", "have",
"it", "its", "of", "one", "for",
"the", "us", "this")
vocab = create_vocabulary(it_train, ngram = c(1L,4L), stopwords = stop_words)
vocab = vocab[vocab$term %in% myvocab, ]
bigram_vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, bigram_vectorizer)
dtm_test = create_dtm(it_test, bigram_vectorizer)
lasso = cv.glmnet(x = dtm_train, y = train$sentiment,
family = 'binomial',
alpha = 1,
type.measure = "auc")
tmp <-predict(lasso, s = lasso$lambda.min, newx = dtm_test,type = "response")
auc(test$sentiment, tmp)
